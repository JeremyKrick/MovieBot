{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /usr/local/lib/python3.11/site-packages (0.27.6)\n",
      "Collecting openai\n",
      "  Obtaining dependency information for openai from https://files.pythonhosted.org/packages/1e/9f/385c25502f437686e4aa715969e5eaf5c2cb5e5ffa7c5cdd52f3c6ae967a/openai-0.28.1-py3-none-any.whl.metadata\n",
      "  Downloading openai-0.28.1-py3-none-any.whl.metadata (11 kB)\n",
      "Requirement already satisfied: requests>=2.20 in /usr/local/lib/python3.11/site-packages (from openai) (2.30.0)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/site-packages (from openai) (4.65.0)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/site-packages (from requests>=2.20->openai) (3.1.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/site-packages (from requests>=2.20->openai) (1.26.15)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/site-packages (from aiohttp->openai) (1.3.1)\n",
      "Downloading openai-0.28.1-py3-none-any.whl (76 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.0/77.0 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: openai\n",
      "  Attempting uninstall: openai\n",
      "    Found existing installation: openai 0.27.6\n",
      "    Uninstalling openai-0.27.6:\n",
      "      Successfully uninstalled openai-0.27.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandasai 0.2.11 requires openai<0.28.0,>=0.27.5, but you have openai 0.28.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed openai-0.28.1\n"
     ]
    }
   ],
   "source": [
    "#!pip install langchain\n",
    "#! pip install --upgrade openai\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "import openai\n",
    "from io import StringIO\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Preprocess Data\n",
    "\n",
    "Our selection of data was narrowed down by our approach to correlate movie actors and the movie lines that they spoke. We used the Cornell Movie-Dialogs Corpus, which is a collection of metadata-rich conversations extracted from raw movie scripts. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset - movie_lines.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line ID</th>\n",
       "      <th>Character ID</th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Character Name</th>\n",
       "      <th>Text of Utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L1044</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>They do to!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L984</td>\n",
       "      <td>u2</td>\n",
       "      <td>m0</td>\n",
       "      <td>CAMERON</td>\n",
       "      <td>She okay?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Line ID Character ID Movie ID Character Name Text of Utterance\n",
       "0   L1045           u0       m0         BIANCA    They do not!\\n\n",
       "1   L1044           u2       m0        CAMERON     They do to!\\n\n",
       "2    L985           u0       m0         BIANCA      I hope so.\\n\n",
       "3    L984           u2       m0        CAMERON       She okay?\\n\n",
       "4    L925           u0       m0         BIANCA       Let's go.\\n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the path to the movie_lines.txt file\n",
    "file_path = 'nlp_group_movie_dataset/movie_lines.txt'\n",
    "# Initialize empty lists to store the data\n",
    "lineID = []\n",
    "characterID = []\n",
    "movieID = []\n",
    "character_name = []\n",
    "text_of_utterance = []\n",
    "# Read first line in the file\n",
    "with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        # Split each line using ' +++$+++ ' as the delimiter\n",
    "        line = line.split(' +++$+++ ')\n",
    "        # Extract the fields\n",
    "        lineID.append(line[0])\n",
    "        characterID.append(line[1])\n",
    "        movieID.append(line[2])\n",
    "        character_name.append(line[3])\n",
    "        text_of_utterance.append(line[4])\n",
    "    f.close()\n",
    "\n",
    "# Create a dataframe from the lists\n",
    "# df = pd.DataFrame({'Line ID': lineID, 'Character ID': characterID, 'Movie ID': movieID, 'Character Name': character_name, 'Text of Utterance': text_of_utterance})\n",
    "df = pd.DataFrame({'Line ID': lineID, 'Character ID': characterID, 'Movie ID': movieID, 'Character Name': character_name, 'Text of Utterance': text_of_utterance})\n",
    "\n",
    "# Display the first 5 rows of the dataframe\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load dataset - movie_characters_metadata.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the path to the movie_lines.txt file\n",
    "file_path = 'nlp_group_movie_dataset/movie_characters_metadata.txt'\n",
    "# Initialize empty lists to store the data\n",
    "#characterID = []\n",
    "character_name = []\n",
    "movieID = []\n",
    "movie_title = []\n",
    "# Read first line in the file\n",
    "with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:\n",
    "    for line in f:\n",
    "        # Split each line using ' +++$+++ ' as the delimiter\n",
    "        line = line.split(' +++$+++ ')\n",
    "        # Extract the fields\n",
    "        # lineID.append(line[0])\n",
    "        # characterID.append(line[1])\n",
    "        character_name.append(line[1])\n",
    "        movieID.append(line[2])\n",
    "        movie_title.append(line[3])\n",
    "    f.close()\n",
    "\n",
    "# Create a dataframe from the lists\n",
    "# df = pd.DataFrame({'Line ID': lineID, 'Character ID': characterID, 'Movie ID': movieID, 'Character Name': character_name, 'Text of Utterance': text_of_utterance})\n",
    "df2 = pd.DataFrame({'Movie ID': movieID, 'Character Name': character_name, 'Movie Title': movie_title})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line ID</th>\n",
       "      <th>Character ID</th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Character Name</th>\n",
       "      <th>Text of Utterance</th>\n",
       "      <th>Movie Title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L872</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Line ID Character ID Movie ID Character Name   \n",
       "0   L1045           u0       m0         BIANCA  \\\n",
       "1    L985           u0       m0         BIANCA   \n",
       "2    L925           u0       m0         BIANCA   \n",
       "3    L872           u0       m0         BIANCA   \n",
       "4    L870           u0       m0         BIANCA   \n",
       "\n",
       "                                   Text of Utterance   \n",
       "0                                     They do not!\\n  \\\n",
       "1                                       I hope so.\\n   \n",
       "2                                        Let's go.\\n   \n",
       "3   Okay -- you're gonna need to learn how to lie.\\n   \n",
       "4  I'm kidding.  You know how sometimes you just ...   \n",
       "\n",
       "                  Movie Title  \n",
       "0  10 things i hate about you  \n",
       "1  10 things i hate about you  \n",
       "2  10 things i hate about you  \n",
       "3  10 things i hate about you  \n",
       "4  10 things i hate about you  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Merge the two DataFrames on 'Movie ID' and 'Character Name'\n",
    "combined_df = pd.merge(df, df2, on=['Movie ID', 'Character Name'], how='outer')\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Line ID</th>\n",
       "      <th>Character ID</th>\n",
       "      <th>Movie ID</th>\n",
       "      <th>Character Name</th>\n",
       "      <th>Text of Utterance</th>\n",
       "      <th>Movie Title</th>\n",
       "      <th>process_utterance</th>\n",
       "      <th>processed_utterance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>L1045</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>They do not!\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>they do not!</td>\n",
       "      <td>they do not!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>L985</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I hope so.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>i hope so.</td>\n",
       "      <td>i hope so.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>L925</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Let's go.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>let's go.</td>\n",
       "      <td>let's go.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>L872</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>Okay -- you're gonna need to learn how to lie.\\n</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>okay  you're gonna need to learn how to lie.</td>\n",
       "      <td>okay  you're gonna need to learn how to lie.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>L870</td>\n",
       "      <td>u0</td>\n",
       "      <td>m0</td>\n",
       "      <td>BIANCA</td>\n",
       "      <td>I'm kidding.  You know how sometimes you just ...</td>\n",
       "      <td>10 things i hate about you</td>\n",
       "      <td>i'm kidding.  you know how sometimes you just ...</td>\n",
       "      <td>i'm kidding.  you know how sometimes you just ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Line ID Character ID Movie ID Character Name   \n",
       "0   L1045           u0       m0         BIANCA  \\\n",
       "1    L985           u0       m0         BIANCA   \n",
       "2    L925           u0       m0         BIANCA   \n",
       "3    L872           u0       m0         BIANCA   \n",
       "4    L870           u0       m0         BIANCA   \n",
       "\n",
       "                                   Text of Utterance   \n",
       "0                                     They do not!\\n  \\\n",
       "1                                       I hope so.\\n   \n",
       "2                                        Let's go.\\n   \n",
       "3   Okay -- you're gonna need to learn how to lie.\\n   \n",
       "4  I'm kidding.  You know how sometimes you just ...   \n",
       "\n",
       "                  Movie Title   \n",
       "0  10 things i hate about you  \\\n",
       "1  10 things i hate about you   \n",
       "2  10 things i hate about you   \n",
       "3  10 things i hate about you   \n",
       "4  10 things i hate about you   \n",
       "\n",
       "                                   process_utterance   \n",
       "0                                       they do not!  \\\n",
       "1                                         i hope so.   \n",
       "2                                          let's go.   \n",
       "3       okay  you're gonna need to learn how to lie.   \n",
       "4  i'm kidding.  you know how sometimes you just ...   \n",
       "\n",
       "                                 processed_utterance  \n",
       "0                                       they do not!  \n",
       "1                                         i hope so.  \n",
       "2                                          let's go.  \n",
       "3       okay  you're gonna need to learn how to lie.  \n",
       "4  i'm kidding.  you know how sometimes you just ...  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preprocess_text(text):\n",
    "    # Convert words to lowercase\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove new lines\n",
    "    text = text.replace('\\n', '')\n",
    "    text = text.replace('\\r', '')\n",
    "    text = text.replace('\\t', '')\n",
    "    text = text.replace('...', '')\n",
    "    text = text.replace('--', '')\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "# Remove unprocessed text\n",
    "combined_df['processed_utterance'] = combined_df['Text of Utterance'].apply(preprocess_text)\n",
    "combined_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "EDA on: movie_lines.txt df\n",
      "Number of unique character names: 5356\n",
      "Number of unique movies: 617\n",
      "Number of unique character IDs: 9035\n",
      "\n",
      "EDA on: movie_characters_metadata.txt df\n",
      "Number of unique character names: 5356\n",
      "Number of unique movies: 617\n",
      "\n",
      "EDA on: Merged dataframe\n",
      "Number of unique character names: 5356\n",
      "Number of unique movies: 617\n",
      "Number of unique character IDs: 9035\n"
     ]
    }
   ],
   "source": [
    "def analyze_dataframe(df, name ):\n",
    "    print(\"\\nEDA on: {}\".format(name))\n",
    "    # interested columns\n",
    "    columns = ['Character Name', 'Movie ID', 'Character ID']\n",
    "    for column in columns:\n",
    "        # check if dataframe contains a column named 'Character Name'\n",
    "        if column in df.columns:        \n",
    "            if column == 'Character Name':        \n",
    "                # Number of unique character names\n",
    "                print('Number of unique character names: {}'.format(df[column].nunique()))\n",
    "            if column == 'Movie ID':\n",
    "                # Number of unique movies\n",
    "                print('Number of unique movies: {}'.format(df[column].nunique()))\n",
    "            if column == 'Character ID':\n",
    "                # Number of unique character IDs\n",
    "                print('Number of unique character IDs: {}'.format(df[column].nunique()))\n",
    "            \n",
    "analyze_dataframe(df, \"movie_lines.txt df\")\n",
    "analyze_dataframe(df2, \"movie_characters_metadata.txt df\")\n",
    "analyze_dataframe(combined_df, \"Merged dataframe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tunning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating data with roles system, user, assistant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_training_ds(df, count= 10):\n",
    "    ds = []\n",
    "    \n",
    "    # give me 10 random samples in the processed utterance\n",
    "    # dynamically insert the movie line, character name, and movie title into the format template\n",
    "    sample = df.sample(count)\n",
    "    for index, row in sample.iterrows():\n",
    "        movie_title = row['Movie Title']\n",
    "        movie_line = row['processed_utterance']\n",
    "        character_name = row['Character Name']\n",
    "\n",
    "        sys_cont = \"Your name Miz and you're a master movie buff. Limited to only movie related questions but leverage all movie content and scripts. Inputs from an example of a raw script are: movie line, character, and movie title in that order. Ex: {movie_line}, {character_name}, {movie_title}\".format(movie_line=movie_line, character_name=character_name, movie_title=movie_title)\n",
    "        user_cont = \"Which character said this line {movie_line}, from the movie {movie_title}?\".format(movie_line=movie_line, movie_title=movie_title)\n",
    "        \n",
    "        format_template = {\"messages\": [\n",
    "            {\"role\": \"system\", \"content\": sys_cont}, \n",
    "            {\"role\": \"user\", \"content\": user_cont},\n",
    "            {\"role\": \"assistant\", \"content\": \"I think it was {character_name} from the movie {movie_title}\".format(character_name=character_name, movie_title=movie_title)},\n",
    "        ]}\n",
    "        ds.append(format_template.copy())\n",
    "    return ds\n",
    "\n",
    "data = create_training_ds(combined_df, 20)\n",
    "\n",
    "data = create_training_ds(combined_df, 15)\n",
    "data2 = create_training_ds(combined_df, 15)\n",
    "\n",
    "def list_to_jsonl(list, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for entry in list:\n",
    "            json_line = json.dumps(entry)  # Convert dict to JSON string\n",
    "            file.write(json_line + '\\n')  # Write JSON string to file with newline\n",
    "    file.close()\n",
    "    return output_file\n",
    "\n",
    "# jsonl_file = list_to_jsonl(data, \"training_data2.jsonl\")\n",
    "\n",
    "training_file = list_to_jsonl(data, \"training_data.jsonl\")\n",
    "validation_file = list_to_jsonl(data2, \"validation_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prompt / Completion Data Creation\n",
    "For fine tunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "2\n",
      "length of jsonl file:  19\n",
      "length of jsonl file:  21\n"
     ]
    }
   ],
   "source": [
    "def create_dataset(df, count= 10):\n",
    "    training_data = {\n",
    "        \"prompt\": [],\n",
    "        \"completion\": []\n",
    "    }\n",
    "    \n",
    "    # give me 10 random samples in the processed utterance\n",
    "    # dynamically insert the movie line, character name, and movie title into the format template\n",
    "    sample = df.sample(count)\n",
    "    for index, row in sample.iterrows():\n",
    "        movie_title = row['Movie Title']\n",
    "        movie_line = row['process_utterance']\n",
    "        character_name = row['Character Name']\n",
    "\n",
    "        prompt = \"Which character in the movie {movie_title} said this line {movie_line}, and what is the movie title if you know? \".format(movie_line=movie_line, movie_title=movie_title)\n",
    "        completion = \"Easy! That was {character_name}, from the movie {movie_title}?\".format(movie_title=movie_title, character_name=character_name) \n",
    "        \n",
    "        training_data[\"prompt\"].append(prompt)\n",
    "        training_data[\"completion\"].append(completion)\n",
    "\n",
    "\n",
    "    return training_data\n",
    "\n",
    "ds1 = create_dataset(combined_df, 15)\n",
    "ds2 = create_dataset(combined_df, 15)\n",
    "\n",
    "def dict_to_jsonl(dictionary, output_file):\n",
    "    with open(output_file, 'w') as file:\n",
    "        for prompt, completion in zip(dictionary[\"prompt\"], dictionary[\"completion\"]):\n",
    "            json_obj = {\"prompt\": prompt, \"completion\": completion}\n",
    "            json_line = json.dumps(json_obj)  # Convert dict to JSON string\n",
    "            file.write(json_line + '\\n')  # Write JSON string to file with newline\n",
    "    file.close()\n",
    "    return output_file\n",
    "\n",
    "\n",
    "training_file_pc = dict_to_jsonl(ds1, \"training_data.jsonl\")\n",
    "validation_file_pc = dict_to_jsonl(ds2, \"validation_data.jsonl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can use file from the previous cell to invote the fine tune job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = openai.File.create(\n",
    "    file=open(training_file,\"rb\"),\n",
    "    purpose='fine-tune'\n",
    ")\n",
    "\n",
    "response2 = openai.File.create(\n",
    "    file=open(validation_file,\"rb\"),\n",
    "    purpose='fine-tune'\n",
    ")\n",
    "\n",
    "training_file_id = response.id\n",
    "validation_file_id = response2.id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not working yet to view status, says an email will be sent to Angel probably"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-NGbOJIsmE7jKfGUZAj6lKuyM\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1697930133,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-n4d01lzaOsa2EPdrmwa4FQAI\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"validating_files\",\n",
      "  \"validation_file\": \"file-HRMBgPx5rsfJ9056GUFI5UZ1\",\n",
      "  \"training_file\": \"file-xP1Evar9Tz1bEMCiIhdf7x60\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ftjob= openai.FineTuningJob.create( training_file=training_file_id,\n",
    "                                    validation_file=validation_file_id,\n",
    "                                    model=\"gpt-3.5-turbo\",\n",
    "                                    hyperparameters={\"n_epochs\":3})\n",
    "\n",
    "print(ftjob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-NGbOJIsmE7jKfGUZAj6lKuyM\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1697930133,\n",
      "  \"finished_at\": null,\n",
      "  \"fine_tuned_model\": null,\n",
      "  \"organization_id\": \"org-n4d01lzaOsa2EPdrmwa4FQAI\",\n",
      "  \"result_files\": [],\n",
      "  \"status\": \"running\",\n",
      "  \"validation_file\": \"file-HRMBgPx5rsfJ9056GUFI5UZ1\",\n",
      "  \"training_file\": \"file-xP1Evar9Tz1bEMCiIhdf7x60\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": null,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Retrieve the state of a fine-tune\n",
    "finetune_job = openai.FineTuningJob.retrieve(ftjob['id'])\n",
    "print(finetune_job)\n",
    "if finetune_job['status'] == 'succeeded':\n",
    "    results_id = finetune_job['result_files'][0]\n",
    "    print(results_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"object\": \"fine_tuning.job\",\n",
      "  \"id\": \"ftjob-NGbOJIsmE7jKfGUZAj6lKuyM\",\n",
      "  \"model\": \"gpt-3.5-turbo-0613\",\n",
      "  \"created_at\": 1697930133,\n",
      "  \"finished_at\": 1697930383,\n",
      "  \"fine_tuned_model\": \"ft:gpt-3.5-turbo-0613:steve-student::8CFXsr70\",\n",
      "  \"organization_id\": \"org-n4d01lzaOsa2EPdrmwa4FQAI\",\n",
      "  \"result_files\": [\n",
      "    \"file-qSBxxbF6pJ80vYVAYS7VuquP\"\n",
      "  ],\n",
      "  \"status\": \"succeeded\",\n",
      "  \"validation_file\": \"file-HRMBgPx5rsfJ9056GUFI5UZ1\",\n",
      "  \"training_file\": \"file-xP1Evar9Tz1bEMCiIhdf7x60\",\n",
      "  \"hyperparameters\": {\n",
      "    \"n_epochs\": 3\n",
      "  },\n",
      "  \"trained_tokens\": 5724,\n",
      "  \"error\": null\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "result_file_metric = openai.FineTuningJob.retrieve(finetune_job['id'])\n",
    "print(result_file_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_file_metric = openai.FineTuningJob.retrieve(finetune_job['id'])\n",
    "result_file_id = result_file_metric['result_files'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>step</th>\n",
       "      <th>train_loss</th>\n",
       "      <th>train_accuracy</th>\n",
       "      <th>valid_loss</th>\n",
       "      <th>valid_mean_token_accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>41</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.71429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>42</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.76471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>43</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.86667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>44</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>0.61538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>45</td>\n",
       "      <td>0.00001</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.00002</td>\n",
       "      <td>0.86667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    step  train_loss  train_accuracy  valid_loss  valid_mean_token_accuracy\n",
       "40    41     0.00001             1.0     0.00001                    0.71429\n",
       "41    42     0.00001             1.0     0.00001                    0.76471\n",
       "42    43     0.00001             1.0     0.00001                    0.86667\n",
       "43    44     0.00001             1.0     0.00001                    0.61538\n",
       "44    45     0.00001             1.0     0.00002                    0.86667"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "content = openai.File.download(result_file_id)    \n",
    "\n",
    "csv_str = content.decode(\"utf-8\")\n",
    "# Convert the CSV string to a Pandas DataFrame\n",
    "csv_buffer = StringIO(csv_str)\n",
    "df = pd.read_csv(csv_buffer)\n",
    "display(df.tail())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interact with the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assistant: I am a helpful assistant and I don't have a personal name. You can just call me \"Assistant\". How can I assist you today?\n",
      "Assistant: \"I amar prestar aen, han mathon ne nen, han mathon ne chae...a line from The Lord of the Rings: The Two Towers movie spoken by Legolas\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb Cell 29\u001b[0m line \u001b[0;36m2\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39m# Continuous interaction loop\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m     \u001b[39m# Prompt the user for a question\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m     user_question \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m(\u001b[39m\"\u001b[39;49m\u001b[39m\\n\u001b[39;49;00m\u001b[39mPlease ask a question (or type \u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mexit\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39m to stop): \u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m     \u001b[39m# Exit condition\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/Steve/dev/GitProjects/Multi-turn-Chatbot-Design/final-group-chatbot.ipynb#X40sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m     \u001b[39mif\u001b[39;00m user_question\u001b[39m.\u001b[39mlower() \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mexit\u001b[39m\u001b[39m'\u001b[39m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1191\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1189\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1190\u001b[0m     \u001b[39mraise\u001b[39;00m StdinNotImplementedError(msg)\n\u001b[0;32m-> 1191\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_input_request(\n\u001b[1;32m   1192\u001b[0m     \u001b[39mstr\u001b[39;49m(prompt),\n\u001b[1;32m   1193\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_parent_ident[\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m],\n\u001b[1;32m   1194\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_parent(\u001b[39m\"\u001b[39;49m\u001b[39mshell\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m   1195\u001b[0m     password\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m   1196\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/ipykernel/kernelbase.py:1234\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1231\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1232\u001b[0m     \u001b[39m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     msg \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mInterrupted by user\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m-> 1234\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mKeyboardInterrupt\u001b[39;00m(msg) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m   1235\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m   1236\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlog\u001b[39m.\u001b[39mwarning(\u001b[39m\"\u001b[39m\u001b[39mInvalid Message:\u001b[39m\u001b[39m\"\u001b[39m, exc_info\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def get_assistant_reply(user_input, system_content='You are a helpful assistant', fine_tune_model=\"gpt-3.5-turbo\"):\n",
    "    # Create a conversation with the model using the user's question\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=fine_tune_model,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": system_content\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": user_input\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    # Extract the assistant's reply from the response\n",
    "    return response['choices'][0]['message']['content']\n",
    "\n",
    "# Continuous interaction loop\n",
    "while True:\n",
    "    # Prompt the user for a question\n",
    "    user_question = input(\"\\nPlease ask a question (or type 'exit' to stop): \")\n",
    "\n",
    "    # Exit condition\n",
    "    if user_question.lower() == 'exit':\n",
    "        print(\"Goodbye!\")\n",
    "        break\n",
    "\n",
    "    assistant_reply = get_assistant_reply(user_question, fine_tune_model=\"ft:gpt-3.5-turbo-0613:steve-student::8CFXsr70\")\n",
    "    print(f\"Assistant: {assistant_reply}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "NLP",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
